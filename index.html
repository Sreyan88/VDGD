<!DOCTYPE html>
<html>
<head>
    <title>VDGD: Reducing Hallucinations in LVLMs</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper-v2.js" defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        td { text-align: right; vertical-align: middle; padding: 10px 24px; }
        th { padding: 10px 24px; }
        audio { display: inline-block; vertical-align: middle; }
        .timestamp-label { color: gray; }
        table { border-spacing: 0 10px; }
        .button-container { display: flex; gap: 20px; justify-content: center; }
        .button { display: flex; align-items: center; padding: 10px 20px; background-color: #333; color: #fff; text-decoration: none; border-radius: 50px; font-size: 16px; font-weight: bold; }
        .button img { margin-right: 10px; width: 20px; height: 20px; }
        .button i { margin-right: 15px; font-size: 20px; }
    </style>
</head>
<body>
    <div class="container pt-5 mt-5">
        <div class="text-center">
            <h1>Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs</h1>
        </div>
        <p align="center"style="font-size: 1.2em;"><b>Sreyan Ghosh<sup>1*</sup>, Chandra Kiran Reddy Evuru<sup>1*</sup>, Sonal Kumar<sup>1*</sup>, Utkarsh Tyagi<sup>1</sup>, Oriol Nieto<sup>2</sup>,  Zeyu Jin<sup>2</sup>, Dinesh Manocha<sup>1</b></p>
        <p align="center" style="font-size: 1.2em;">
            <sup>1</sup>University of Maryland, <sup>2</sup>Adobe
        </p>
            
        
        <div class="button-container">
            <a href="https://arxiv.org/pdf/2405.15683" class="button"><i class="fas fa-file-pdf"></i> Paper</a>
            <a href="https://github.com/Sreyan88/VDGD" class="button"><i class="fab fa-github"></i> Code</a>
            <a href="https://drive.google.com/file/d/1QPISJ_2qszTeopQEnR3jmA31n4qTlYsG/view" class="button"><i class="fab fa-database"></i> VaLLu</a>
        </div>
        <br>
        
        <h2>Abstract</h2>
        <p>Large Vision-Language Models (LVLMs) often produce responses that misalign with factual information, a phenomenon known as hallucinations. While hallucinations are well-studied, the exact causes behind them remain underexplored. In this paper, we first investigate the root causes of hallucinations in LVLMs. Our findings reveal that existing mitigation techniques primarily reduce hallucinations for visual recognition prompts—those that require simple descriptions of visual elements—but fail for cognitive prompts that demand deliberate reasoning. We identify the core issue as a lack of true visual perception in LVLMs: although they can accurately recognize visual elements, they struggle to fully interpret these elements in the context of the input prompt and effectively link this recognition to their internal knowledge, which is critical for reasoning. To address this gap, we introduce Visual Description Grounded Decoding (VDGD), a simple, robust, and *training-free* method designed to enhance visual perception and improve reasoning capabilities in LVLMs. VDGD works by first generating a detailed description of the image and appending it as a prefix to the instruction. During response generation, tokens are sampled based on their KL divergence to the description, favoring candidates with lower divergence. Experimental results on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD consistently outperforms existing baselines  2% - 33%. Finally, we introduce VaLLu, a benchmark designed for comprehensive evaluation of the cognitive capabilities of LVLMs.</p>
        
        <h2>Key Findings</h2>
        <h3>1. Existing Hallucination Mitigation Fails for Cognitive Prompts</h3>
        <p>While prior techniques work well for visual recognition tasks, they fail when applied to cognitive prompts requiring reasoning.</p>
        <figure>
            <p align="center"><img src="static/radar1-2-1.png" width="70%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 1:</b>
                (Left) Performance comparison of different LVLMs on various benchmarks. (Right) Performance comparison of different hallucination mitigation techniques applied to LLaVA-1.5.
              </p>
            </figcaption>
        </figure>        
        
        <h3>2. Visual Hallucinations Exist in Multiple Forms</h3>
        <p>We categorize hallucinations into four types: Language, Vision, Style, and Instruction Tuning (IT). Existing methods only mitigate a subset.</p>
        <figure>
            <p align="center"><img src="static/vllm.drawio5.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 2:</b>
                    Types of Visual Recognition Hallucinations.
              </p>
            </figcaption>
        </figure>

        <h3>3. LVLMs Have a Visual Perception Gap</h3>
        <p>LVLMs can recognize visual elements but struggle to link them with internal knowledge, leading to incorrect reasoning.</p>
        <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
            <figure style="text-align: center; width: 40%;">
                <img src="static/TR_line_cogvlm-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 3:</b> Base Rank Comparison between AMBER and MATH-Vision datasets as a function of token position in responses (for CogVLM).
                </figcaption>
            </figure>
        
            <figure style="text-align: center; width: 55%;">
                <img src="static/lm_radar_combined-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 4:</b> (Left) Performance comparison of different LVLMs when prompted w/ original prompt vs rephrased prompts w/o image (-t). (Right) Performance comparison of different LVLMs for their ability to generate a faithful image description.
                </figcaption>
            </figure>
        </div>
        
        
        <h3>4. VDGD Bridges the Perception Gap</h3>
        <p>By generating a detailed image description and using KL divergence to guide response generation, VDGD reduces hallucinations and improves reasoning.</p>
        <figure>
            <p align="center"><img src="static/vllm-kl4.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 2:</b>
                    Illustration of our proposed VDGD method.
              </p>
            </figcaption>
        </figure>
        
        <h2>Results</h2>
        <p>We evaluate VDGD on multiple benchmarks, demonstrating improvements of 2%-33% over existing techniques.</p>
        <figure>
            <p align="center"><img src="static/vllm-table.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Table 1:</b>
                    Performance comparison of VDGD with various baselines. VDGD outperforms by 2%-33%.
              </p>
            </figcaption>
        </figure>
    </div>
</body>
</html>
