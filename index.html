<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VDGD: Reducing Hallucinations in LVLMs</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="helper-v2.js" defer></script>
  <style>
    td {
      text-align: right;
      vertical-align: middle;
      padding: 10px 24px;
    }
    th {
      padding: 10px 24px;
    }
    audio {
      display: inline-block;
      vertical-align: middle;
    }
    .timestamp-label {
      color: gray;
    }
    table {
      border-spacing: 0 10px;
    }
    .button-container {
      display: flex;
      gap: 20px;
      justify-content: center;
    }
    .button {
      display: flex;
      align-items: center;
      padding: 10px 20px;
      background-color: #333;
      color: #fff;
      text-decoration: none;
      border-radius: 50px;
      font-size: 16px;
      font-weight: bold;
    }
    .button img {
      margin-right: 10px;
      width: 20px;
      height: 20px;
    }
    .button i {
      margin-right: 15px;
      font-size: 20px;
    }
  </style>
</head>
<body>
  <div class="container pt-5 mt-5">
    <div class="text-center">
      <h1>Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs</h1>
    </div>
    <p class="text-center" style="font-size: 1.2em;">
      <b>
        Sreyan Ghosh<sup>1*</sup>, Chandra Kiran Reddy Evuru<sup>1*</sup>, Sonal Kumar<sup>1*</sup>, Utkarsh Tyagi<sup>1</sup>, Oriol Nieto<sup>2</sup>, Zeyu Jin<sup>2</sup>, Dinesh Manocha<sup>1</sup>
      </b>
    </p>
    <p class="text-center" style="font-size: 1.2em;">
      <sup>1</sup>University of Maryland, <sup>2</sup>Adobe
    </p>

    <div class="button-container">
      <a href="https://arxiv.org/pdf/2405.15683" class="button">
        <i class="fas fa-file-pdf"></i> Paper
      </a>
      <a href="https://github.com/Sreyan88/VDGD" class="button">
        <i class="fab fa-github"></i> Code
      </a>
      <a href="https://drive.google.com/file/d/1QPISJ_2qszTeopQEnR3jmA31n4qTlYsG/view" class="button">
        <i class="fab fa-database"></i> VaLLu
      </a>
    </div>
    <br>

    <h2 class="mt-4">Abstract</h2>
    <p class="mb-4">
      Large Vision-Language Models (LVLMs) often produce responses that misalign with factual information, a phenomenon known as hallucinations. While hallucinations are well-studied, the exact causes behind them remain underexplored. In this paper, we first investigate the root causes of hallucinations in LVLMs. Our findings reveal that existing mitigation techniques primarily reduce hallucinations for visual recognition prompts—those that require simple descriptions of visual elements—but fail for cognitive prompts that demand deliberate reasoning. We identify the core issue as a lack of true visual perception in LVLMs: although they can accurately recognize visual elements, they struggle to fully interpret these elements in the context of the input prompt and effectively link this recognition to their internal knowledge, which is critical for reasoning. To address this gap, we introduce Visual Description Grounded Decoding (VDGD), a simple, robust, and <em>training-free</em> method designed to enhance visual perception and improve reasoning capabilities in LVLMs. VDGD works by first generating a detailed description of the image and appending it as a prefix to the instruction. During response generation, tokens are sampled based on their KL divergence to the description, favoring candidates with lower divergence. Experimental results on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD consistently outperforms existing baselines by 2% - 33%. Finally, we introduce VaLLu, a benchmark designed for comprehensive evaluation of the cognitive capabilities of LVLMs.
    </p>

    <h2 class="mt-4">Key Findings</h2>
    <h3>1. Existing Hallucination Mitigation Fails for Cognitive Prompts</h3>
    <p>
      While prior techniques work well for visual recognition tasks, they fail when applied to cognitive prompts requiring reasoning.
    </p>
    <figure class="text-center">
      <img src="static/radar1-2-1.png" width="70%" alt="Performance comparison chart" class="center">
      <figcaption class="text-center">
        <b>Figure 1:</b> (Left) Performance comparison of different LVLMs on various benchmarks. (Right) Performance comparison of different hallucination mitigation techniques applied to LLaVA-1.5.
      </figcaption>
    </figure>

    <h3 class="mt-4">2. Visual Hallucinations Exist in Multiple Forms</h3>
    <p class="mb-4">
      We categorize hallucinations into four types: Language, Vision, Style, and Instruction Tuning (IT). Existing methods only mitigate a subset.
    </p>
    <figure class="text-center">
      <img src="static/categories.png" width="75%" alt="Hallucination categories" class="center">
      <figcaption class="text-center">
        <b>Figure 2:</b> Types of Visual Recognition Hallucinations.
      </figcaption>
    </figure>

    <h3 class="mt-4">3. LVLMs Have a Visual Perception Gap</h3>
    <p class="mb-4">
      LVLMs can recognize visual elements but struggle to link them with internal knowledge, leading to incorrect reasoning.
    </p>
    <div class="d-flex justify-content-center align-items-center gap-3">
      <figure class="text-center" style="width: 40%;">
        <img src="static/TR_line_cogvlm-1.png" style="width: 100%;" alt="Base Rank Comparison">
        <figcaption>
          <b>Figure 3:</b> Base Rank Comparison between AMBER and MATH-Vision datasets as a function of token position in responses (for CogVLM).
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 55%;">
        <img src="static/lm_radar_combined-1.png" style="width: 100%;" alt="Performance comparison">
        <figcaption>
          <b>Figure 4:</b> (Left) Performance comparison of different LVLMs when prompted with the original prompt vs. rephrased prompts without image (-t). (Right) Performance comparison of different LVLMs for their ability to generate a faithful image description.
        </figcaption>
      </figure>
    </div>

    <h3 class="mt-4">4. VDGD Bridges the Perception Gap</h3>
    <p class="mb-4">
      By generating a detailed image description and using KL divergence to guide response generation, VDGD reduces hallucinations and improves reasoning.
    </p>
    <figure class="text-center">
      <img src="static/vllm-kl4.png" width="75%" alt="VDGD method illustration" class="center">
      <figcaption class="text-center">
        <b>Figure 5:</b> Illustration of our proposed VDGD method.
      </figcaption>
    </figure>

    <h2 class="mt-4">5. Differences between VDGD and Other Baselines</h2>
    <table class="table table-bordered text-center" style="margin: 0 auto;">
      <thead>
        <tr>
          <th>Aspect</th>
          <th>VDGD</th>
          <th>Other Baselines</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1. Training Requirement</td>
          <td>
            <b>Training-free</b> approach that appends a detailed image description and uses KL divergence for robust decoding.
          </td>
          <td>
            Often require additional training, fine-tuning, or specialized modules to address object hallucinations.
          </td>
        </tr>
        <tr>
          <td>2. Scope of Mitigation</td>
          <td>
            Targets <b>all forms</b> of hallucinations—especially those in <em>cognitive prompts</em>—by bridging the “visual perception gap.”
          </td>
          <td>
            Primarily reduce <b>object-based</b> or “visual recognition” hallucinations; limited efficacy on more complex, <em>reasoning-intensive</em> tasks.
          </td>
        </tr>
        <tr>
          <td>3. Performance Gains</td>
          <td>
            Consistently outperforms baselines by <b>2%–33%</b> on multiple benchmarks, improving both <em>reasoning</em> and recognition accuracy.
          </td>
          <td>
            Show <b>smaller</b> or <em>no gains</em> on cognitive prompts requiring extended reasoning or domain knowledge.
          </td>
        </tr>
      </tbody>
    </table>

    <h2 class="mt-4">6. Results</h2>
    <p class="mb-4">
      We evaluate VDGD on multiple benchmarks, demonstrating improvements of 2%-33% over existing techniques.
    </p>
    <figure class="text-center">
      <img src="static/vllm-table.png" width="75%" alt="Results table" class="center">
      <figcaption class="text-center">
        <b>Table 1:</b> Performance comparison of VDGD with various baselines. VDGD outperforms by 2%-33%.
      </figcaption>
    </figure>

    <h2 class="mt-4">7. Qualitative Comparison of Greedy vs VCD vs VDGD Decoding Methods</h2>
    <p class="mb-4">
      We illustrate several instances from VaLLu and compare their responses for LLaVA-1.5 with Greedy, VCD, and VDGD Decoding.
    </p>
    <div class="d-flex justify-content-center align-items-center gap-3">
      <figure class="text-center" style="width: 45%;">
        <img src="static/example1.png" style="width: 100%;" alt="Qualitative example 1">
        <figcaption>
          <b>Figure 6:</b> Qualitative Example 1
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 45%;">
        <img src="static/example2.png" style="width: 100%;" alt="Qualitative example 2">
        <figcaption>
          <b>Figure 7:</b> Qualitative Example 2
        </figcaption>
      </figure>
    </div>

    <h2 class="mt-4">7. The VaLLu Benchmark</h2>
    <p class="mb-4">
      VaLLu consists of 1,500 instances sourced from multiple benchmarks, including Oven, MMMU, MMC, MathVista, HallusionBench, MATH-Vision, and MME. It focuses exclusively on open-ended generation tasks, excluding Yes/No and multiple-choice questions, to focus on evaluating diverse forms of hallucination by LVLMs. The dataset is carefully curated to balance affordability and task diversity, ensuring a comprehensive evaluation. Additionally, VaLLu undergoes manual filtering to remove noisy samples (we find existing benchmarks to have noisy samples as shown below) and is enriched with meta-data annotations and expert-provided responses for high-quality benchmarking.
    </p>
    <figure class="text-center">
      <img src="static/chart-11-1.png" width="55%" alt="Task type distribution" class="center">
      <figcaption class="text-center">
        <b>Figure 8:</b> Distribution of task types in VaLLu.
      </figcaption>
    </figure>

    <div class="d-flex justify-content-center align-items-center gap-3 mt-4">
      <figure class="text-center" style="width: 50%;">
        <img src="static/hallu_noisy_img_1-compressed-1.png" style="width: 100%;" alt="Noisy example from HallusionBench">
        <figcaption>
          <b>Figure 9:</b> Noisy example from HallusionBench
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 50%;">
        <img src="static/mathvista_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MathVista">
        <figcaption>
          <b>Figure 10:</b> Noisy example from MathVista
        </figcaption>
      </figure>
    </div>

    <div class="d-flex justify-content-center align-items-center gap-3 mt-4">
      <figure class="text-center" style="width: 50%;">
        <img src="static/mmc_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MMC">
        <figcaption>
          <b>Figure 11:</b> Noisy example from MMC
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 50%;">
        <img src="static/mmmu_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MMMU">
        <figcaption>
          <b>Figure 12:</b> Noisy examples from MMMU
        </figcaption>
      </figure>
    </div>

  </div>
</body>
</html>
