<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs</title>
  <style>
      * {
          margin: 0;
          padding: 0;
          box-sizing: border-box;
      }

      body {
          font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
          line-height: 1.6;
          color: #333;
          background-color: #f8f9fa;
          max-width: 1200px;
          margin: 0 auto;
          padding: 20px;
      }

      header {
          text-align: center;
          padding: 40px 20px 30px;
          margin-bottom: 30px;
          background: white;
          color: black;
          border-radius: 10px;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
      }

      /* Space for logos in header */
      .logo-section {
        margin: 20px 0;
      }
      .logo-section img {
        height: 60px;
        margin: 0 20px;
      }

      /* Single definition of .finding (no display:flex here) */
      .finding {
          background-color: white;
          padding: 25px;
          border-radius: 10px;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
          margin-bottom: 25px;
      }

      .finding-text {
          margin-bottom: 20px;
      }

      .finding-text p {
          font-size: 1.05rem;
          line-height: 1.7;
          margin-bottom: 0;
      }

      .diagram-container {
          display: flex;
          justify-content: space-between;
          align-items: flex-start;
          gap: 25px;
      }

      .diagram-figure {
          flex: 1;
          text-align: center;
          margin: 0;
      }

      .diagram-figure img {
          width: 100%;
          border-radius: 8px;
          box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
          margin-bottom: 10px;
      }

      .diagram-figure figcaption {
          font-size: 0.9rem;
          color: #555;
          line-height: 1.4;
          text-align: center;
      }

      .diagram-figure figcaption b {
          color: #2c3e50;
      }

      h1 {
          font-size: 2.6rem;
          margin-bottom: 15px;
          font-weight: 700;
      }

      .authors {
          font-size: 1.2rem;
          margin: 15px 0;
          font-weight: 500;
      }

      .affiliations {
          font-size: 1rem;
          margin-bottom: 20px;
          opacity: 0.9;
      }

      .buttons {
          display: flex;
          justify-content: center;
          gap: 20px;
          margin: 25px 0 10px;
      }

      .btn {
          display: inline-block;
          padding: 10px 25px;
          background-color: rgba(255, 255, 255, 0.15);
          color: black;
          border: 2px solid black;
          border-radius: 30px;
          font-weight: 600;
          text-decoration: none;
          transition: all 0.3s ease;
      }

      .btn:hover {
          background-color: white;
          color: #4b6cb7;
          transform: translateY(-2px);
          box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      }

      .abstract {
          background-color: white;
          padding: 35px;
          border-radius: 10px;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
          margin-bottom: 40px;
          text-align: justify;
      }

      h2 {
          font-size: 1.8rem;
          margin: 35px 0 20px;
          color: #2c3e50;
          padding-bottom: 10px;
          border-bottom: 2px solid #4b6cb7;
      }

      h3 {
          font-size: 1.4rem;
          margin: 25px 0 15px;
          color: #34495e;
      }

      p {
          margin-bottom: 20px;
          font-size: 1.05rem;
          line-height: 1.7;
      }

      em {
          font-style: italic;
          font-weight: 500;
          color: #e74c3c;
      }

      .key-findings {
          margin: 40px 0;
      }

      /* Each main finding “block” can use .section for styling */
      .section {
          background-color: white;
          padding: 25px;
          border-radius: 10px;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
          margin-bottom: 25px;
      }

      .finding-image {
          text-align: center;
          padding: 10px;
      }

      .finding-image img {
          max-width: 100%;
          border-radius: 8px;
          box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
      }

      .image_caption {
          text-align: center;
          font-size: 0.9rem;
          color: #555;
          line-height: 1.4;
          margin-top: 5px;
      }

      table {
          width: 100%;
          border-collapse: collapse;
          margin: 30px 0;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
          border-radius: 10px;
          overflow: hidden;
      }

      th, td {
          padding: 15px;
          text-align: left;
          border-bottom: 1px solid #eee;
      }

      th {
          background-color: #2c3e50;
          color: white;
          font-weight: 600;
      }

      tr:nth-child(even) {
          background-color: #f5f9fc;
      }

      .section-image {
          text-align: center;
          margin: 30px 0;
      }

      .section-image img {
          max-width: 100%;
          border-radius: 8px;
          box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
      }

      footer {
          text-align: center;
          margin-top: 60px;
          padding: 20px;
          color: #7f8c8d;
          font-size: 0.9rem;
          border-top: 1px solid #eee;
      }

      /* Basic flex utilities since no external bootstrap is used */
      .d-flex {
        display: flex;
      }
      .justify-content-center {
        justify-content: center;
      }
      .align-items-center {
        align-items: center;
      }
      .gap-3 {
        gap: 1rem;
      }
      .text-center {
        text-align: center;
      }
      .mt-4 {
        margin-top: 1.5rem;
      }
  </style>
</head>
<body>
  <header>
    <!-- Top row: UMD logo - Title - Adobe logo -->
    <div style="display: flex; justify-content: space-between; align-items: center;">
      <img src="static/umdlogo.png" alt="UMD Logo" style="height: 120px;">
      <h1>Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs</h1>
      <img src="static/adobelogo.png" alt="Adobe Logo" style="height: 120px;">
    </div>

    <!-- Authors & affiliations below -->
    <div class="authors">
      Sreyan Ghosh<sup>1*</sup>, Chandra Kiran Reddy Evuru<sup>1*</sup>, Sonal Kumar<sup>1*</sup>,
      Utkarsh Tyagi<sup>1</sup>, Oriol Nieto<sup>2</sup>, Zeyu Jin<sup>2</sup>,
      Dinesh Manocha<sup>1</sup>
    </div>
    <div class="affiliations">
      <sup>1</sup>University of Maryland, <sup>2</sup>Adobe<br>
      *Equal contribution
    </div>

    <div class="buttons">
      <a href="https://arxiv.org/pdf/2405.15683" class="btn">Paper</a>
      <a href="https://github.com/Sreyan88/VDGD" class="btn">Code</a>
      <a href="https://drive.google.com/file/d/1QPISJ_2qszTeopQEnR3jmA31n4qTlYsG/view" class="btn">VaLLu</a>
    </div>
  </header>

  <div class="abstract">
    <p>
      Large Vision-Language Models (LVLMs) often produce responses that misalign with factual information, a phenomenon known as hallucinations. While hallucinations are well-studied, the exact causes behind them remain underexplored. In this paper, we first investigate the root causes of hallucinations in LVLMs. Our findings reveal that existing mitigation techniques primarily reduce hallucinations for visual recognition prompts—those that require simple descriptions of visual elements—but fail for cognitive prompts that demand deliberate reasoning.
    </p>
    <p>
      We identify the core issue as a lack of true visual perception in LVLMs: although they can accurately recognize visual elements, they struggle to fully interpret these elements in the context of the input prompt and effectively link this recognition to their internal knowledge, which is critical for reasoning. To address this gap, we introduce Visual Description Grounded Decoding (VDGD), a simple, robust, and <em>training-free</em> method designed to enhance visual perception and improve reasoning capabilities in LVLMs.
    </p>
    <p>
      VDGD works by first generating a detailed description of the image and appending it as a prefix to the instruction. During response generation, tokens are sampled based on their KL divergence to the description, favoring candidates with lower divergence. Experimental results on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD consistently outperforms existing baselines by 2% - 33%. Finally, we introduce VaLLu, a benchmark designed for comprehensive evaluation of the cognitive capabilities of LVLMs.
    </p>
  </div>

  <h2>Key Findings</h2>
  <div class="key-findings">
    <!-- Finding 1 -->
    <div class="section">
      <div class="finding-text">
        <p>1. While prior techniques work well for visual recognition tasks, they fail when applied to cognitive prompts requiring reasoning.</p>
      </div>
      <div class="finding-image">
        <img src="static/radar1-2-1.png" alt="Visual Recognition vs Cognitive Tasks">
      </div>
      <div class="image_caption">
        <b>Figure 1:</b> (Left) Performance comparison of different LVLMs on various benchmarks. (Right) Performance comparison of different hallucination mitigation techniques applied to LLaVA-1.5.
      </div>
    </div>

    <!-- Finding 2 -->
    <div class="section">
      <div class="finding-text">
        <p>2. We categorize hallucinations into four types: Language, Vision, Style, and Instruction Tuning (IT). Existing methods only mitigate a subset.</p>
      </div>
      <div class="finding-image">
        <img src="static/categories.png" alt="Hallucination Types">
      </div>
      <div class="image_caption">
        <b>Figure 2:</b> Types of Visual Recognition Hallucinations.
      </div>
    </div>

    <!-- Finding 3 -->
    <div class="section">
      <div class="finding-text">
        <p>3. LVLMs can recognize visual elements but struggle to link them with internal knowledge, leading to incorrect reasoning.</p>
      </div>
      <div style="display: flex; gap: 25px; flex-wrap: wrap; justify-content: space-around;">
        <div style="width: 340px; max-width: 100%; text-align: center;">
          <img src="static/TR_line_cogvlm-1.png" alt="Base Rank Comparison" style="max-width: 100%;">
          <div class="image_caption">
            <b>Figure 3:</b> Base Rank Comparison between AMBER and MATH-Vision datasets as a function of token position in responses (for CogVLM).
          </div>
        </div>
        <div style="width: 340px; max-width: 100%; text-align: center;">
          <img src="static/lm_radar_combined-1.png" alt="Performance comparison" style="max-width: 100%;">
          <div class="image_caption">
            <b>Figure 4:</b> (Left) Performance comparison of different LVLMs when prompted with the original prompt vs. rephrased prompts without image (-t). (Right) Performance comparison of different LVLMs for their ability to generate a faithful image description.
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Our Approach -->
  <div class="section">
    <h2>Our Approach</h2>
    <p>
      VDGD works by first generating a detailed description of the image and appending it as a prefix to the instruction. During response generation, tokens are sampled based on their KL divergence to the description, favoring candidates with lower divergence.
    </p>
    <div class="section-image">
      <img src="static/vllm-kl4.png" alt="VDGD Approach Diagram">
    </div>
    <div class="image_caption">
      <b>Figure 5:</b> Illustration of our proposed VDGD method.
    </div>
  </div>

  <!-- Comparison with Existing Methods -->
  <div class="section">
    <h2>Comparison with Existing Methods</h2>
    <table>
      <thead>
        <tr>
          <th>Aspect</th>
          <th>VDGD</th>
          <th>Other Baselines</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1. Training Requirement</td>
          <td>Training-free approach that appends a detailed image description and uses KL divergence for robust decoding.</td>
          <td>Often require additional training, fine-tuning, or specialized modules to address object hallucinations.</td>
        </tr>
        <tr>
          <td>2. Scope of Mitigation</td>
          <td>Targets all forms of hallucinations—especially those in cognitive prompts—by bridging the "visual perception gap."</td>
          <td>Primarily reduce object-based or "visual recognition" hallucinations; limited efficacy on more complex, reasoning-intensive tasks.</td>
        </tr>
        <tr>
          <td>3. Performance Gains</td>
          <td>Consistently outperforms baselines by 2%–33% on multiple benchmarks, improving both reasoning and recognition accuracy.</td>
          <td>Show smaller or no gains on cognitive prompts requiring extended reasoning or domain knowledge.</td>
        </tr>
      </tbody>
    </table>
  </div>

  <!-- Evaluation Results -->
  <div class="section">
    <h2>Evaluation Results</h2>
    <p>We evaluate VDGD on multiple benchmarks, demonstrating improvements of 2%-33% over existing techniques.</p>
    <div class="section-image">
      <img src="static/vllm-table.png" alt="Evaluation Results">
    </div>
    <div class="image_caption">
      <b>Figure 6:</b> Evaluation Results across multiple benchmarks.
    </div>
  </div>

  <!-- Qualitative Analysis (images aligned horizontally) -->
  <div class="section">
    <h2>Qualitative Analysis</h2>
    <p>We illustrate several instances from VaLLu and compare their responses for LLaVA-1.5 with Greedy, VCD, and VDGD Decoding.</p>
    <div class="d-flex justify-content-center align-items-center gap-3">
      <figure class="text-center" style="width: 45%;">
        <img src="static/example1.png" style="width: 100%;" alt="Qualitative example 1">
        <figcaption>
          <b>Figure 6:</b> Qualitative Example 1
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 45%;">
        <img src="static/example2.png" style="width: 100%;" alt="Qualitative example 2">
        <figcaption>
          <b>Figure 7:</b> Qualitative Example 2
        </figcaption>
      </figure>
    </div>
  </div>

  <!-- VaLLu Benchmark -->
  <div class="section">
    <h2>The VaLLu Benchmark</h2>
    <p>
      VaLLu consists of 1,500 instances sourced from multiple benchmarks, including Oven, MMMU, MMC, MathVista, HallusionBench, MATH-Vision, and MME. It focuses exclusively on open-ended generation tasks, excluding Yes/No and multiple-choice questions, to focus on evaluating diverse forms of hallucination by LVLMs.
    </p>
    <p>
      The dataset is carefully curated to balance affordability and task diversity, ensuring a comprehensive evaluation. Additionally, VaLLu undergoes manual filtering to remove noisy samples (we find existing benchmarks to have noisy samples as shown below) and is enriched with meta-data annotations and expert-provided responses for high-quality benchmarking.
    </p>
    <!-- Figure 8 centered -->
    <figure class="text-center">
      <img src="static/chart-11-1.png" width="55%" alt="Task type distribution" class="center">
      <figcaption class="text-center">
        <b>Figure 8:</b> Distribution of task types in VaLLu.
      </figcaption>
    </figure>

    <!-- Figures 9,10,11,12 in 2x2 orientation -->
    <div class="d-flex justify-content-center align-items-center gap-3 mt-4">
      <figure class="text-center" style="width: 50%;">
        <img src="static/hallu_noisy_img_1-compressed-1.png" style="width: 100%;" alt="Noisy example from HallusionBench">
        <figcaption>
          <b>Figure 9:</b> Noisy example from HallusionBench
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 50%;">
        <img src="static/mathvista_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MathVista">
        <figcaption>
          <b>Figure 10:</b> Noisy example from MathVista
        </figcaption>
      </figure>
    </div>

    <div class="d-flex justify-content-center align-items-center gap-3 mt-4">
      <figure class="text-center" style="width: 50%;">
        <img src="static/mmc_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MMC">
        <figcaption>
          <b>Figure 11:</b> Noisy example from MMC
        </figcaption>
      </figure>
      <figure class="text-center" style="width: 50%;">
        <img src="static/mmmu_noisy_img_1-1.png" style="width: 100%;" alt="Noisy example from MMMU">
        <figcaption>
          <b>Figure 12:</b> Noisy examples from MMMU
        </figcaption>
      </figure>
    </div>
  </div>

  <footer>
    &copy; 2025 University of Maryland and Adobe Research
  </footer>
</body>
</html>