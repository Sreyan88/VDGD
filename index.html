<!DOCTYPE html>
<html>
<head>
    <title>VDGD: Reducing Hallucinations in LVLMs</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper-v2.js" defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        td { text-align: right; vertical-align: middle; padding: 10px 24px; }
        th { padding: 10px 24px; }
        audio { display: inline-block; vertical-align: middle; }
        .timestamp-label { color: gray; }
        table { border-spacing: 0 10px; }
        .button-container { display: flex; gap: 20px; justify-content: center; }
        .button { display: flex; align-items: center; padding: 10px 20px; background-color: #333; color: #fff; text-decoration: none; border-radius: 50px; font-size: 16px; font-weight: bold; }
        .button img { margin-right: 10px; width: 20px; height: 20px; }
        .button i { margin-right: 15px; font-size: 20px; }
    </style>
</head>
<body>
    <div class="container pt-5 mt-5">
        <div class="text-center">
            <h1>Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs</h1>
        </div>
        <p align="center"style="font-size: 1.2em;"><b>Sreyan Ghosh<sup>1*</sup>, Chandra Kiran Reddy Evuru<sup>1*</sup>, Sonal Kumar<sup>1*</sup>, Utkarsh Tyagi<sup>1</sup>, Oriol Nieto<sup>2</sup>,  Zeyu Jin<sup>2</sup>, Dinesh Manocha<sup>1</b></p>
        <p align="center" style="font-size: 1.2em;">
            <sup>1</sup>University of Maryland, <sup>2</sup>Adobe
        </p>


        <div class="button-container">
            <a href="https://arxiv.org/pdf/2405.15683" class="button"><i class="fas fa-file-pdf"></i> Paper</a>
            <a href="https://github.com/Sreyan88/VDGD" class="button"><i class="fab fa-github"></i> Code</a>
            <a href="https://drive.google.com/file/d/1QPISJ_2qszTeopQEnR3jmA31n4qTlYsG/view" class="button"><i class="fab fa-database"></i> VaLLu</a>
        </div>
        <br>

        <h2 style="margin-top: 40px;">Abstract</h2>
        <p style="margin-bottom: 20px;">Large Vision-Language Models (LVLMs) often produce responses that misalign with factual information, a phenomenon known as hallucinations. While hallucinations are well-studied, the exact causes behind them remain underexplored. In this paper, we first investigate the root causes of hallucinations in LVLMs. Our findings reveal that existing mitigation techniques primarily reduce hallucinations for visual recognition prompts—those that require simple descriptions of visual elements—but fail for cognitive prompts that demand deliberate reasoning. We identify the core issue as a lack of true visual perception in LVLMs: although they can accurately recognize visual elements, they struggle to fully interpret these elements in the context of the input prompt and effectively link this recognition to their internal knowledge, which is critical for reasoning. To address this gap, we introduce Visual Description Grounded Decoding (VDGD), a simple, robust, and *training-free* method designed to enhance visual perception and improve reasoning capabilities in LVLMs. VDGD works by first generating a detailed description of the image and appending it as a prefix to the instruction. During response generation, tokens are sampled based on their KL divergence to the description, favoring candidates with lower divergence. Experimental results on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD consistently outperforms existing baselines  2% - 33%. Finally, we introduce VaLLu, a benchmark designed for comprehensive evaluation of the cognitive capabilities of LVLMs.</p>

        <h2 style="margin-top: 40px;">Key Findings</h2>
        <h3>1. Existing Hallucination Mitigation Fails for Cognitive Prompts</h3>
        <p>While prior techniques work well for visual recognition tasks, they fail when applied to cognitive prompts requiring reasoning.</p>
        <figure>
            <p align="center"><img src="static/radar1-2-1.png" width="70%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 1:</b>
                (Left) Performance comparison of different LVLMs on various benchmarks. (Right) Performance comparison of different hallucination mitigation techniques applied to LLaVA-1.5.
              </p>
            </figcaption>
        </figure>

        <h3 style="margin-top: 40px;">2. Visual Hallucinations Exist in Multiple Forms</h3>
        <p style="margin-bottom: 20px;">We categorize hallucinations into four types: Language, Vision, Style, and Instruction Tuning (IT). Existing methods only mitigate a subset.</p>
        <figure>
            <p align="center"><img src="static/categories.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 2:</b>
                    Types of Visual Recognition Hallucinations.
              </p>
            </figcaption>
        </figure>

        <h3 style="margin-top: 40px;">3. LVLMs Have a Visual Perception Gap</h3>
        <p style="margin-bottom: 20px;">LVLMs can recognize visual elements but struggle to link them with internal knowledge, leading to incorrect reasoning.</p>
        <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
            <figure style="text-align: center; width: 40%;">
                <img src="static/TR_line_cogvlm-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 3:</b> Base Rank Comparison between AMBER and MATH-Vision datasets as a function of token position in responses (for CogVLM).
                </figcaption>
            </figure>

            <figure style="text-align: center; width: 55%;">
                <img src="static/lm_radar_combined-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 4:</b> (Left) Performance comparison of different LVLMs when prompted w/ original prompt vs rephrased prompts w/o image (-t). (Right) Performance comparison of different LVLMs for their ability to generate a faithful image description.
                </figcaption>
            </figure>
        </div>

        <div />


        <h3 style="margin-top: 40px;">4. VDGD Bridges the Perception Gap</h3>
        <p style="margin-bottom: 20px;">By generating a detailed image description and using KL divergence to guide response generation, VDGD reduces hallucinations and improves reasoning.</p>
        <figure>
            <p align="center"><img src="static/vllm-kl4.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 5:</b>
                    Illustration of our proposed VDGD method.
              </p>
            </figcaption>
        </figure>

        <h2 style="margin-top: 40px;">5. Differences between VDGD and Other Baselines</h2>
        <table class="table table-bordered" style="margin: 0 auto; text-align: center;">
            <thead>
                <tr>
                    <th>Aspect</th>
                    <th>VDGD</th>
                    <th>Other Baselines</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1. Training Requirement</td>
                    <td>
                        <b>Training-free</b> approach that appends a detailed image description
                        and uses KL divergence for robust decoding
                    </td>
                    <td>
                        Often require additional training, fine-tuning, or specialized
                        modules to address object hallucinations
                    </td>
                </tr>
                <tr>
                    <td>2. Scope of Mitigation</td>
                    <td>
                        Targets <b>all forms</b> of hallucinations—especially those in
                        <em>cognitive prompts</em>—by bridging the “visual perception gap”
                    </td>
                    <td>
                        Primarily reduce <b>object-based</b> or “visual recognition” hallucinations;
                        limited efficacy on more complex, <em>reasoning-intensive</em> tasks
                    </td>
                </tr>
                <tr>
                    <td>3. Performance Gains</td>
                    <td>
                        Consistently outperforms baselines by <b>2%–33%</b> on multiple benchmarks,
                        improving both <em>reasoning</em> and recognition accuracy
                    </td>
                    <td>
                        Show <b>smaller</b> or <em>no gains</em> on cognitive prompts requiring
                        extended reasoning or domain knowledge
                    </td>
                </tr>
            </tbody>
        </table>

        <h2 style="margin-top: 40px;">6. Results</h2>
        <p style="margin-bottom: 20px;">We evaluate VDGD on multiple benchmarks, demonstrating improvements of 2%-33% over existing techniques.</p>
        <figure>
            <p align="center"><img src="static/vllm-table.png" width="75%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Table 1:</b>
                    Performance comparison of VDGD with various baselines. VDGD outperforms by 2%-33%.
              </p>
            </figcaption>
        </figure>

        <h2 style="margin-top: 40px;">7. Qualitative comparison of Greedy vs VCD vs VDGD Decoding Methods</h2>
        <p style="margin-bottom: 20px;">We illustrate several instances from VaLLu and compare their responses for LLaVa-1.5 with Greedy, VCD, and VDGD Decoding.</p>
        <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
            <figure style="text-align: center; width: 45%;">
                <img src="static/example1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 6:</b> Qualitative Example 1
                </figcaption>
            </figure>

            <figure style="text-align: center; width: 45%;">
                <img src="static/example2.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 7:</b> Qualitative Example 2
                </figcaption>
            </figure>
        </div>

        <h2 style="margin-top: 40px;">7. The VaLLu Benchmark</h2>
        <p style="margin-bottom: 20px;">VaLLu consists of 1,500 instances sourced from multiple benchmarks, including Oven, MMMU, MMC, MathVista, HallusionBench, MATH-Vision, and MME. It focuses exclusively on open-ended generation tasks, excluding Yes/No and multiple-choice questions, to focus on evaluating diverse forms of halluicnation by LVLMs. The dataset is carefully curated to balance affordability and task diversity, ensuring a comprehensive evaluation. Additionally, VaLLu undergoes manual filtering to remove noisy samples (we find existing benchmarks to have noisy samples as shown below) and is enriched with meta-data annotations and expert-provided responses for high-quality benchmarking.</p>
        <figure>
            <p align="center"><img src="static/chart-11-1.png" width="55%" class="center" /></p>
            <figcaption>
              <p style="text-align: center;">
                <font color="000000"><b>Figure 8:</b>
                    Distribution of task types in VaLLu.
              </p>
            </figcaption>
        </figure>
        <div style="display: flex; justify-content: center; align-items: center; gap: 20px; margin-top: 40px;">
            <figure style="text-align: center; width: 50%;">
                <img src="static/hallu_noisy_img_1-compressed-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 9:</b> Noisy example from HallusionBench
                </figcaption>
            </figure>

            <figure style="text-align: center; width: 50%;">
                <img src="static/mathvista_noisy_img_1-1.png" style="width: 100%; margin-top: 40px;" />
                <figcaption>
                    <b>Figure 10:</b> Noisy example from MathVista
                </figcaption>
            </figure>
        </div>

        <div style="display: flex; justify-content: center; align-items: center; gap: 20px; margin-top: 40px;">
            <figure style="text-align: center; width: 50%;">
                <img src="static/mmc_noisy_img_1-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 11:</b> Noisy example from MMC
                </figcaption>
            </figure>

            <figure style="text-align: center; width: 50%;">
                <img src="static/mmmu_noisy_img_1-1.png" style="width: 100%;" />
                <figcaption>
                    <b>Figure 12:</b> Noisy examples from MMMU
                </figcaption>
            </figure>
        </div>

    </div>
</body>
</html>
